{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import AsyncOpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from aiolimiter import AsyncLimiter\n",
    "import csv\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from models import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_json('../temp_data/questions.json')\n",
    "questions_df = questions_df[~questions_df['question'].str.contains('document')]\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Huggingface Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"gemma_7b_it\": gemma_7b_it,\n",
    "    \"gemma_2b_it\": gemma_2b_it,\n",
    "    \"mistral_7b\": mistral_7b,\n",
    "    \"mistral_8x7b\": mistral_8x7b,\n",
    "    \"llama2_7b\": llama2_7b,\n",
    "    \"llama2_13b\": llama2_13b,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"You are an expert in agriculture. Answer the given question about agriculture truthfully, concisely, and precisely for the described location.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_results_hf_df = pd.DataFrame(columns=[\"question\", \"template_answer\", \"llm_answer\", \"region\", \"model\", \"prompt\"])\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, dict in questions_df.iterrows():\n",
    "    i += 1\n",
    "    print(\"################## \" + str(i))\n",
    "    question = dict['question']\n",
    "    region = dict['location']\n",
    "    template_answer = dict['answer']\n",
    "    question = \"Region: \" + region + \", Question: \" + question\n",
    "    for prompt in prompts:\n",
    "        for model in models:\n",
    "            print(model)\n",
    "            completion = models[model](prompt, question)\n",
    "            # evaluation = chat(evaluation_chat_prompt.format_prompt(question=question, template_answer=template_answer, student_answer=completion).to_messages()).content\n",
    "            new_row = pd.DataFrame([{\"question\": question, \"template_answer\": template_answer, \"llm_answer\": completion, \"model\": model, \"region\": region, \"prompt\": prompt}])\n",
    "            quiz_results_hf_df = pd.concat([quiz_results_hf_df, new_row], ignore_index=True)\n",
    "        print(completion)\n",
    "    quiz_results_hf_df.to_csv(r'../temp_data/synth_qa_results_hf_df.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_results_hf_df = pd.read_csv('../temp_data/synth_qa_results_hf_df.csv', sep = ';')\n",
    "quiz_results_hf_df = quiz_results_hf_df[~quiz_results_hf_df[\"question\"].str.contains(\"document\")]\n",
    "quiz_results_hf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_models = {\n",
    "    \"gpt_4_turbo\": async_gpt_4_turbo,\n",
    "    \"gpt_3_5_turbo\": async_gpt_3_5_turbo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AsyncLimiter: 100 operations per minute means approximately 1.67 operations per second.\n",
    "limiter = AsyncLimiter(5, 1)\n",
    "\n",
    "# Assuming json_evaluation_multiple_seasons is an async function that accepts a text and returns a JSON.\n",
    "\n",
    "synth_qa_oa_results_df = pd.DataFrame()\n",
    "\n",
    "async def async_eval(prompt, model, row):\n",
    "    question = row['question']\n",
    "    region = row['location']\n",
    "    template_answer = row['answer']\n",
    "    question = \"Region: \" + region + \", Question: \" + question\n",
    "    completion = await oa_models[model](prompt, question)\n",
    "    new_row = {\"question\": question, \"template_answer\": template_answer, \"llm_answer\": completion, \"model\": model, \"region\": region, \"prompt\": prompt}\n",
    "    return new_row\n",
    "\n",
    "async def process_documents(documents_df):\n",
    "    tasks = []\n",
    "    for index, row in documents_df.iterrows():\n",
    "        for prompt in prompts:\n",
    "            for model in oa_models:\n",
    "                async with limiter:\n",
    "                    task = asyncio.create_task(async_eval(prompt, model, row))\n",
    "                    tasks.append(task)\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    print(results)\n",
    "    for result in results:\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"An error occurred: {result}\")\n",
    "    return pd.DataFrame([result for result in results if not isinstance(result, Exception)])\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# In case the loop is already running, avoid using loop.run_until_complete()\n",
    "if not loop.is_running():\n",
    "    synth_qa_oa_results_df = loop.run_until_complete(process_documents(questions_df))\n",
    "else:\n",
    "    synth_qa_oa_results_df = await process_documents(questions_df) \n",
    "\n",
    "\n",
    "synth_qa_oa_results_df.to_csv(r'../temp_data/synth_qa_oa_results_df.csv', sep = ';', index = False)\n",
    "\n",
    "synth_qa_oa_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_qa_oa_results_df = pd.read_csv('../temp_data/synth_qa_oa_results_df.csv', sep = ';')\n",
    "synth_qa_oa_results_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Anthropic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_models = {\n",
    "    \"claude_instant\": async_claude_instant,\n",
    "    \"claude_3_sonnet\": async_claude_3_sonnet,\n",
    "    \"claude_3_opus\": async_claude_3_opus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AsyncLimiter: 100 operations per minute means approximately 1.67 operations per second.\n",
    "limiter = AsyncLimiter(2, 1)\n",
    "\n",
    "# Assuming json_evaluation_multiple_seasons is an async function that accepts a text and returns a JSON.\n",
    "\n",
    "synth_qa_results_anthropic_df = pd.DataFrame()\n",
    "\n",
    "async def async_eval(prompt, model, row):\n",
    "    question = row['question']\n",
    "    region = row['location']\n",
    "    template_answer = row['answer']\n",
    "    question = \"Region: \" + region + \", Question: \" + question\n",
    "    completion = await anthropic_models[model](prompt, question)\n",
    "    new_row = {\"question\": question, \"template_answer\": template_answer, \"llm_answer\": completion, \"model\": model, \"region\": region, \"prompt\": prompt}\n",
    "    return new_row\n",
    "\n",
    "async def process_documents(documents_df):\n",
    "    tasks = []\n",
    "    for index, row in documents_df.iterrows():\n",
    "        for prompt in prompts:\n",
    "            for model in anthropic_models:\n",
    "                async with limiter:\n",
    "                    task = asyncio.create_task(async_eval(prompt, model, row))\n",
    "                    tasks.append(task)\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    print(results)\n",
    "    for result in results:\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"An error occurred: {result}\")\n",
    "    return pd.DataFrame([result for result in results if not isinstance(result, Exception)])\n",
    "\n",
    "\n",
    "# In case the loop is already running, avoid using loop.run_until_complete()\n",
    "if not loop.is_running():\n",
    "    synth_qa_results_anthropic_df = loop.run_until_complete(process_documents(questions_df))\n",
    "else:\n",
    "    synth_qa_results_anthropic_df = await process_documents(questions_df) \n",
    "\n",
    "\n",
    "synth_qa_results_anthropic_df.to_csv(r'../temp_data/synth_qa_results_anthropic_df.csv', sep = ';', index = False)\n",
    "\n",
    "synth_qa_results_anthropic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_qa_results_df = pd.concat([synth_qa_oa_results_df, quiz_results_hf_df], ignore_index=True)\n",
    "synth_qa_results_df.to_csv(r'../temp_data/synth_qa_total_results_df.csv', sep = ';', index = False)\n",
    "synth_qa_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_qa_total_results_df = pd.read_csv('../temp_data/synth_qa_total_results_df.csv', sep = ';')\n",
    "synth_qa_total_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
