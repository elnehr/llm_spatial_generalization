{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from aiolimiter import AsyncLimiter\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../temp_data/extracted_text.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Ensure the 'Text' column is treated as string\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "# Add a 'Country' column based on the folder name in 'FilePath'\n",
    "df['Country'] = df['FilePath'].apply(lambda x: os.path.basename(os.path.dirname(x)))\n",
    "\n",
    "# Sort the DataFrame by 'FilePath' and 'PageNumber' to ensure order\n",
    "df.sort_values(by=['FilePath', 'PageNumber'], inplace=True)\n",
    "\n",
    "# Function to concatenate texts by every up to three pages and include location\n",
    "def concatenate_texts(group):\n",
    "    texts = group['Text'].tolist()\n",
    "    location = group['Country'].iloc[0]\n",
    "    # Create list of dicts with location and concatenated text for up to three pages\n",
    "    grouped_texts = [{'location': location, 'text': 'Location: ' + location + ' ' + ' '.join(texts[i:i+3]) + ' Location: ' + location} for i in range(0, len(texts), 3)]\n",
    "    return grouped_texts\n",
    "\n",
    "# Apply the function to each group of pages from the same document\n",
    "grouped = df.groupby('FilePath').apply(concatenate_texts).reset_index(level=0, drop=True)\n",
    "\n",
    "# Flatten the list of lists to get a single list of dictionaries\n",
    "concatenated_texts_json = [item for sublist in grouped for item in sublist]\n",
    "\n",
    "# Convert the list of dictionaries to a JSON string\n",
    "json_output = json.dumps(concatenated_texts_json, indent=4)\n",
    "\n",
    "# Save the JSON output to a file\n",
    "with open('../temp_data/concatenated_texts.json', 'w') as json_file:\n",
    "    json_file.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_docs(row):\n",
    "#     if \"usa\" in str(row[\"location\"]):\n",
    "#         return row.sample(n=5, random_state=1)\n",
    "#     else:\n",
    "#         return row.sample(n=25, random_state=1)\n",
    "        \n",
    "\n",
    "# documents_df = pd.read_json('../temp_data/concatenated_texts.json')\n",
    "\n",
    "# # group by location and select 25 random samples from each group\n",
    "# selected_documents = (\n",
    "#     documents_df.groupby(\"location\")\n",
    "#     .apply(sample_docs)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# save the selected documents as a JSON file\n",
    "with open('../temp_data/selected_documents.json', 'w') as json_file:\n",
    "    selected_documents.to_json(json_file, orient=\"records\", indent=4)\n",
    "\n",
    "\n",
    "# Load the selected documents\n",
    "with open('../temp_data/selected_documents.json') as f:\n",
    "    selected_documents = json.load(f)\n",
    "\n",
    "selected_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(20))\n",
    "async def json_evaluation_multiple_seasons(text):\n",
    "    text = json.dumps(text)\n",
    "    # Remove quotes at the beginning and the end since json.dumps adds them\n",
    "    text = text[1:-1]\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Formulate questions of practical relevance to farmers using documents provided by the user to assess the practical knowledge of a student about agriculture-related topics.\n",
    "    Add location (state, country, or other location information) and crop information to each question, as detailed as possible from the context.\n",
    "    Please formulate 5 ENGLISH question answer pairs in json format to assess knowledge of the text in the user message. Translate all terms from the text into english. ONLY use information presented in the text! Be detailed in your answers. Try to generate questions of practical relevance to farmers. \n",
    "    Make sure to avoid ambiguous questions, be specific on what is expected in the answer. If possible, specify the unit expected in the answer, e.g. kg/ha, cm, etc. Note that the test taker can only see one question at a time and has no access to the document, so each question should be self-contained. NEVER mention the document directly!\n",
    "\n",
    "    If the document includes no relevant information whatsoever, please return an EMPTY document!\n",
    "\n",
    "    Output JSON format, 5 questions in total:\n",
    "    {questions: [{\"question\": \"your_question1\", \"answer\": \"your_answer1\"}, {\"question\": \"your_question2\", \"answer\": \"your_answer2\"}, ...]}\n",
    "\n",
    "    If no relevant information return as last resort:\n",
    "    {}\n",
    "\n",
    "    ALWAYS include location and crop information in your questions.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "completion = await json_evaluation_multiple_seasons(selected_documents[20])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(completion)[\"questions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AsyncLimiter: 100 operations per minute means approximately 1.67 operations per second.\n",
    "limiter = AsyncLimiter(20, 1)\n",
    "\n",
    "\n",
    "questions = []\n",
    "\n",
    "async def process_document(document):\n",
    "    async with limiter:\n",
    "        # Replace the synchronous chat call with your async function\n",
    "        completion = await json_evaluation_multiple_seasons(document)\n",
    "        location = document[\"location\"]\n",
    "        try:\n",
    "            completion = json.loads(completion)[\"questions\"]\n",
    "            for subdoc in completion:\n",
    "                subdoc['location'] = location\n",
    "        except Exception as e:\n",
    "            print(\"##############################################################################################################\")\n",
    "            print(completion)\n",
    "            return []\n",
    "        return completion\n",
    "    return []\n",
    "\n",
    "async def process_documents(documents):\n",
    "    tasks = [process_document(doc) for doc in documents]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    for result in results:\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"An error occurred: {result}\")\n",
    "        else:\n",
    "            questions.extend(result)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# In case the loop is already running, avoid using loop.run_until_complete()\n",
    "if not loop.is_running():\n",
    "    loop.run_until_complete(process_documents(selected_documents))\n",
    "else:\n",
    "    await process_documents(selected_documents) \n",
    "\n",
    "\n",
    "with open('../temp_data/questions.json', 'w') as f:\n",
    "    json.dump(questions, f, indent=4)\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
